import re
import json
import os
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
import PyPDF2
import pdfplumber
from pathlib import Path
import logging
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BuildingCodeClause:
    """Represents a single building code clause"""
    clause_number: str
    title: str
    content: str
    page_number: int
    document_source: str
    clause_level: int
    parent_clause: Optional[str]
    subsections: List[str]
    related_clauses: List[str]
    tables: List[str]
    figures: List[str]
    metadata: Dict

@dataclass
class ProcessedDocument:
    """Represents a processed building code document"""
    document_name: str
    document_type: str
    total_pages: int
    processing_date: str
    clauses: List[BuildingCodeClause]
    document_metadata: Dict

class BuildingCodePDFProcessor:
    def __init__(self):
        # Regex patterns for building code structure
        self.clause_patterns = {
            # Main clauses like D1, E2, B1, etc.
            'main_clause': re.compile(r'^([A-Z]\d+)\s+(.+?)(?:\n|$)', re.MULTILINE),
            
            # Sub-clauses like D1.1, E2.3, etc.
            'sub_clause': re.compile(r'^([A-Z]\d+\.\d+)\s+(.+?)(?:\n|$)', re.MULTILINE),
            
            # Sub-sub-clauses like D1.3.3, E2.1.4, etc.
            'sub_sub_clause': re.compile(r'^([A-Z]\d+\.\d+\.\d+)\s+(.+?)(?:\n|$)', re.MULTILINE),
            
            # Performance criteria like D1/AS1, E2/AS2
            'performance_clause': re.compile(r'^([A-Z]\d+/AS\d+)\s+(.+?)(?:\n|$)', re.MULTILINE),
            
            # Verification methods like D1/VM1
            'verification_clause': re.compile(r'^([A-Z]\d+/VM\d+)\s+(.+?)(?:\n|$)', re.MULTILINE),
            
            # Building Act sections
            'act_section': re.compile(r'^(Section\s+\d+[A-Z]?)\s+(.+?)(?:\n|$)', re.MULTILINE),
            
            # Schedule references
            'schedule': re.compile(r'^(Schedule\s+\d+[A-Z]?)\s+(.+?)(?:\n|$)', re.MULTILINE)
        }
        
        # Patterns for related content
        self.content_patterns = {
            'table': re.compile(r'(Table\s+[A-Z]?\d+(?:\.\d+)*(?:[a-z])?)', re.IGNORECASE),
            'figure': re.compile(r'(Figure\s+[A-Z]?\d+(?:\.\d+)*(?:[a-z])?)', re.IGNORECASE),
            'cross_reference': re.compile(r'([A-Z]\d+(?:\.\d+)*(?:\.\d+)*)', re.MULTILINE),
            'definition': re.compile(r'^([A-Z][a-z\s]+):\s+(.+)', re.MULTILINE)
        }

    def extract_text_with_metadata(self, pdf_path: str) -> Dict:
        """Extract text from PDF while preserving structure and metadata"""
        extracted_data = {
            'pages': [],
            'metadata': {},
            'total_pages': 0
        }
        
        try:
            # Use pdfplumber for better text extraction
            with pdfplumber.open(pdf_path) as pdf:
                extracted_data['total_pages'] = len(pdf.pages)
                extracted_data['metadata'] = pdf.metadata or {}
                
                for page_num, page in enumerate(pdf.pages, 1):
                    page_data = {
                        'page_number': page_num,
                        'text': page.extract_text() or '',
                        'tables': page.extract_tables() or [],
                        'bbox': page.bbox,
                        'width': page.width,
                        'height': page.height
                    }
                    extracted_data['pages'].append(page_data)
                    
        except Exception as e:
            logger.error(f"Error extracting text from {pdf_path}: {e}")
            # Fallback to PyPDF2
            try:
                with open(pdf_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    extracted_data['total_pages'] = len(pdf_reader.pages)
                    
                    for page_num, page in enumerate(pdf_reader.pages, 1):
                        page_text = page.extract_text() or ''
                        page_data = {
                            'page_number': page_num,
                            'text': page_text,
                            'tables': [],
                            'bbox': None,
                            'width': None,
                            'height': None
                        }
                        extracted_data['pages'].append(page_data)
            except Exception as e2:
                logger.error(f"Fallback extraction also failed: {e2}")
                raise
                
        return extracted_data

    def identify_document_type(self, text: str, metadata: Dict) -> str:
        """Identify the type of building document"""
        text_lower = text.lower()
        
        # Check for specific document types
        if 'building code' in text_lower and 'clause' in text_lower:
            return 'building_code'
        elif 'building act' in text_lower:
            return 'building_act'
        elif 'acceptable solution' in text_lower:
            return 'acceptable_solution'
        elif 'verification method' in text_lower:
            return 'verification_method'
        elif 'guidance' in text_lower or 'guidance document' in text_lower:
            return 'guidance_document'
        elif 'amendment' in text_lower:
            return 'amendment'
        else:
            return 'unknown'

    def extract_clauses(self, extracted_data: Dict) -> List[BuildingCodeClause]:
        """Extract and structure building code clauses"""
        clauses = []
        full_text = '\n'.join([page['text'] for page in extracted_data['pages']])
        
        # Find all clause matches
        all_matches = []
        
        for pattern_name, pattern in self.clause_patterns.items():
            for match in pattern.finditer(full_text):
                clause_number = match.group(1)
                title = match.group(2).strip()
                start_pos = match.start()
                
                # Find which page this clause is on
                page_number = self.find_page_for_position(extracted_data['pages'], start_pos)
                
                all_matches.append({
                    'clause_number': clause_number,
                    'title': title,
                    'start_pos': start_pos,
                    'page_number': page_number,
                    'pattern_type': pattern_name
                })
        
        # Sort matches by position
        all_matches.sort(key=lambda x: x['start_pos'])
        
        # Extract content for each clause
        for i, match in enumerate(all_matches):
            try:
                # Determine content boundaries
                start_pos = match['start_pos']
                if i + 1 < len(all_matches):
                    end_pos = all_matches[i + 1]['start_pos']
                else:
                    end_pos = len(full_text)
                
                content = full_text[start_pos:end_pos].strip()
                
                # Clean and structure the content
                content = self.clean_clause_content(content)
                
                # Extract metadata
                clause_metadata = self.extract_clause_metadata(content)
                
                clause = BuildingCodeClause(
                    clause_number=match['clause_number'],
                    title=match['title'],
                    content=content,
                    page_number=match['page_number'],
                    document_source=extracted_data['metadata'].get('title', ''),
                    clause_level=self.determine_clause_level(match['clause_number']),
                    parent_clause=self.find_parent_clause(match['clause_number']),
                    subsections=self.extract_subsections(content),
                    related_clauses=self.find_related_clauses(content),
                    tables=self.extract_tables_references(content),
                    figures=self.extract_figure_references(content),
                    metadata=clause_metadata
                )
                
                clauses.append(clause)
                
            except Exception as e:
                logger.error(f"Error processing clause {match['clause_number']}: {e}")
                continue
        
        return clauses

    def find_page_for_position(self, pages: List[Dict], position: int) -> int:
        """Find which page a text position belongs to"""
        current_pos = 0
        for page in pages:
            page_text_length = len(page['text'])
            if current_pos <= position < current_pos + page_text_length:
                return page['page_number']
            current_pos += page_text_length + 1  # +1 for newline between pages
        return 1  # Default to first page

    def clean_clause_content(self, content: str) -> str:
        """Clean and format clause content"""
        # Remove excessive whitespace
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = re.sub(r'[ \t]+', ' ', content)
        
        # Fix common OCR issues
        content = content.replace('', "'")  # Smart quotes
        content = content.replace('', "'")
        content = content.replace('', '"')
        content = content.replace('', '"')
        
        return content.strip()

    def determine_clause_level(self, clause_number: str) -> int:
        """Determine the hierarchical level of a clause"""
        if re.match(r'^[A-Z]\d+$', clause_number):
            return 1  # Main clause (e.g., D1)
        elif re.match(r'^[A-Z]\d+\.\d+$', clause_number):
            return 2  # Sub-clause (e.g., D1.3)
        elif re.match(r'^[A-Z]\d+\.\d+\.\d+$', clause_number):
            return 3  # Sub-sub-clause (e.g., D1.3.3)
        elif re.match(r'^[A-Z]\d+/[A-Z]+\d+$', clause_number):
            return 2  # Performance/verification (e.g., D1/AS1)
        else:
            return 0  # Unknown or other

    def find_parent_clause(self, clause_number: str) -> Optional[str]:
        """Find the parent clause for a given clause"""
        if re.match(r'^[A-Z]\d+\.\d+\.\d+$', clause_number):
            # Sub-sub-clause: parent is sub-clause
            return '.'.join(clause_number.split('.')[:-1])
        elif re.match(r'^[A-Z]\d+\.\d+$', clause_number):
            # Sub-clause: parent is main clause
            return clause_number.split('.')[0]
        else:
            return None

    def extract_subsections(self, content: str) -> List[str]:
        """Extract subsection references from content"""
        subsections = []
        
        # Look for lettered subsections (a), (b), (c), etc.
        letter_subsections = re.findall(r'\(([a-z])\)', content)
        subsections.extend(letter_subsections)
        
        # Look for numbered subsections (i), (ii), (iii), etc.
        roman_subsections = re.findall(r'\(([ivx]+)\)', content)
        subsections.extend(roman_subsections)
        
        return list(set(subsections))

    def find_related_clauses(self, content: str) -> List[str]:
        """Find references to other clauses in the content"""
        related_clauses = []
        
        # Find clause references
        clause_refs = self.content_patterns['cross_reference'].findall(content)
        for ref in clause_refs:
            if re.match(r'^[A-Z]\d+(?:\.\d+)*$', ref):
                related_clauses.append(ref)
        
        return list(set(related_clauses))

    def extract_tables_references(self, content: str) -> List[str]:
        """Extract table references from content"""
        return self.content_patterns['table'].findall(content)

    def extract_figure_references(self, content: str) -> List[str]:
        """Extract figure references from content"""
        return self.content_patterns['figure'].findall(content)

    def extract_clause_metadata(self, content: str) -> Dict:
        """Extract metadata from clause content"""
        metadata = {
            'word_count': len(content.split()),
            'has_tables': bool(self.content_patterns['table'].search(content)),
            'has_figures': bool(self.content_patterns['figure'].search(content)),
            'has_cross_references': bool(self.content_patterns['cross_reference'].search(content)),
            'content_type': self.determine_content_type(content)
        }
        
        return metadata

    def determine_content_type(self, content: str) -> str:
        """Determine the type of content in a clause"""
        content_lower = content.lower()
        
        if 'performance' in content_lower and 'shall' in content_lower:
            return 'performance_requirement'
        elif 'acceptable solution' in content_lower:
            return 'acceptable_solution'
        elif 'verification method' in content_lower:
            return 'verification_method'
        elif 'definition' in content_lower or 'means' in content_lower:
            return 'definition'
        elif 'table' in content_lower:
            return 'table_reference'
        else:
            return 'general_content'

    def create_chunks_for_rag(self, clauses: List[BuildingCodeClause]) -> List[Dict]:
        """Create optimized chunks for RAG system"""
        chunks = []
        
        for clause in clauses:
            # Main clause chunk
            main_chunk = {
                'id': f"{clause.document_source}_{clause.clause_number}",
                'clause_number': clause.clause_number,
                'title': clause.title,
                'content': clause.content,
                'page_number': clause.page_number,
                'document_source': clause.document_source,
                'chunk_type': 'main_clause',
                'metadata': {
                    'clause_level': clause.clause_level,
                    'parent_clause': clause.parent_clause,
                    'related_clauses': clause.related_clauses,
                    'tables': clause.tables,
                    'figures': clause.figures,
                    **clause.metadata
                }
            }
            chunks.append(main_chunk)
            
            # Create additional chunks for long content
            if len(clause.content) > 2000:  # If content is very long
                paragraphs = clause.content.split('\n\n')
                for i, paragraph in enumerate(paragraphs):
                    if len(paragraph.strip()) > 100:  # Only meaningful paragraphs
                        sub_chunk = {
                            'id': f"{clause.document_source}_{clause.clause_number}_p{i+1}",
                            'clause_number': clause.clause_number,
                            'title': f"{clause.title} (Part {i+1})",
                            'content': paragraph.strip(),
                            'page_number': clause.page_number,
                            'document_source': clause.document_source,
                            'chunk_type': 'paragraph',
                            'metadata': {
                                'parent_clause_id': f"{clause.document_source}_{clause.clause_number}",
                                'paragraph_index': i+1,
                                **clause.metadata
                            }
                        }
                        chunks.append(sub_chunk)
        
        return chunks

    def process_pdf(self, pdf_path: str, output_dir: str = None) -> ProcessedDocument:
        """Main method to process a building code PDF"""
        logger.info(f"Processing PDF: {pdf_path}")
        
        # Extract text and metadata
        extracted_data = self.extract_text_with_metadata(pdf_path)
        
        # Identify document type
        full_text = '\n'.join([page['text'] for page in extracted_data['pages']])
        doc_type = self.identify_document_type(full_text, extracted_data['metadata'])
        
        # Extract clauses
        clauses = self.extract_clauses(extracted_data)
        
        logger.info(f"Extracted {len(clauses)} clauses from {Path(pdf_path).name}")
        
        # Create processed document
        processed_doc = ProcessedDocument(
            document_name=Path(pdf_path).name,
            document_type=doc_type,
            total_pages=extracted_data['total_pages'],
            processing_date=datetime.now().isoformat(),
            clauses=clauses,
            document_metadata=extracted_data['metadata']
        )
        
        # Save processed data if output directory specified
        if output_dir:
            self.save_processed_document(processed_doc, output_dir)
        
        return processed_doc

    def save_processed_document(self, doc: ProcessedDocument, output_dir: str):
        """Save processed document to JSON files"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Save full document
        doc_path = os.path.join(output_dir, f"{doc.document_name}_processed.json")
        with open(doc_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(doc), f, indent=2, ensure_ascii=False)
        
        # Save RAG chunks
        chunks = self.create_chunks_for_rag(doc.clauses)
        chunks_path = os.path.join(output_dir, f"{doc.document_name}_chunks.json")
        with open(chunks_path, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved processed document to {output_dir}")

    def batch_process_pdfs(self, input_dir: str, output_dir: str) -> List[ProcessedDocument]:
        """Process multiple PDFs in a directory"""
        pdf_files = list(Path(input_dir).glob("*.pdf"))
        processed_docs = []
        
        logger.info(f"Found {len(pdf_files)} PDF files to process")
        
        for pdf_file in pdf_files:
            try:
                processed_doc = self.process_pdf(str(pdf_file), output_dir)
                processed_docs.append(processed_doc)
            except Exception as e:
                logger.error(f"Error processing {pdf_file}: {e}")
                continue
        
        return processed_docs

# Example usage and integration class
class RAGIntegration:
    """Integration class for RAG systems"""
    
    def __init__(self, processor: BuildingCodePDFProcessor):
        self.processor = processor
    
    def process_new_document(self, pdf_path: str) -> Dict:
        """Process a new document and return RAG-ready chunks"""
        processed_doc = self.processor.process_pdf(pdf_path)
        chunks = self.processor.create_chunks_for_rag(processed_doc.clauses)
        
        return {
            'document_info': {
                'name': processed_doc.document_name,
                'type': processed_doc.document_type,
                'total_pages': processed_doc.total_pages,
                'total_clauses': len(processed_doc.clauses),
                'processing_date': processed_doc.processing_date
            },
            'chunks': chunks
        }
    
    def update_vector_database(self, chunks: List[Dict], vector_db_client):
        """Update vector database with new chunks"""
        # This would integrate with your specific vector database
        # Examples for different databases:
        
        # For Pinecone:
        # vectors = [(chunk['id'], embedding, chunk['metadata']) for chunk in chunks]
        # vector_db_client.upsert(vectors=vectors)
        
        # For Chroma:
        # vector_db_client.add(
        #     documents=[chunk['content'] for chunk in chunks],
        #     metadatas=[chunk['metadata'] for chunk in chunks],
        #     ids=[chunk['id'] for chunk in chunks]
        # )
        
        pass

if __name__ == "__main__":
    # Example usage
    processor = BuildingCodePDFProcessor()
    
    # Process a single PDF
    # processed_doc = processor.process_pdf("building_code_d1.pdf", "output/")
    
    # Process multiple PDFs
    # processed_docs = processor.batch_process_pdfs("input_pdfs/", "output/")
    
    # Integration with RAG
    # rag_integration = RAGIntegration(processor)
    # rag_data = rag_integration.process_new_document("building_code_d1.pdf")
    # print(f"Created {len(rag_data['chunks'])} chunks for RAG system")
    
    print("PDF Processing Pipeline ready!")